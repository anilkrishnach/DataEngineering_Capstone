{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import configparser\n",
    "from pyspark.sql.types import ArrayType, IntegerType, DateType\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import udf, col, count, monotonically_increasing_id, substring, ceil\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format\n",
    "from zipfile import ZipFile\n",
    "import glob\n",
    "import datetime\n",
    "import os\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Timer for logging\n",
    "time = datetime.datetime.now()\n",
    "def get_time():\n",
    "    global time\n",
    "    new_time = datetime.datetime.now()\n",
    "    seconds = (new_time-time).seconds\n",
    "    time=new_time\n",
    "    return seconds\n",
    "\n",
    "# Setting logging attributes\n",
    "logging.basicConfig(filename='ETL.log', filemode='w', format='%(message)s - %(created)f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# For AWS keys\n",
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['DEFAULT']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['DEFAULT']['AWS_SECRET_ACCESS_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Creating a spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "    .getOrCreate()\n",
    "logging.info('Spark session created.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Population data\n",
    "logging.info('Starting population data read.') \n",
    "get_time()\n",
    "population_pd = pd.read_csv(\"http://www2.census.gov/programs-surveys/popest/datasets/2010-2019/national/totals/nst-est2019-alldata.csv\")\n",
    "logging.info('Population data staged. Time taken: {} Row count: {}'.format(get_time(), population_pd.count())) \n",
    "\n",
    "population_staging = spark.createDataFrame(population_pd)\n",
    "logging.info('Cleaning Data and selecting necessary columns.')\n",
    "\n",
    "population_dimension = population_staging.filter(population_staging['state'].between(1,56)).select('state', 'name', 'popestimate2016', 'popestimate2017', 'popestimate2018', 'popestimate2019').withColumnRenamed('popestimate2016', 'population_2016').withColumnRenamed('popestimate2017', 'population_2017').withColumnRenamed('popestimate2018', 'population_2018').withColumnRenamed('popestimate2019', 'population_2019')\n",
    "population_dimension.createOrReplaceTempView('population_dimension')\n",
    "logging.info('Population dimension dataframe and view created. Row count: {}'.format(population_dimension.count()))\n",
    "\n",
    "population_dimension.write.parquet(\"population\", \"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(state=1, name='Alabama', population_2016=4863525, population_2017=4874486, population_2018=4887681, population_2019=4903185)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This cell be could be utilised for ensuring proper data load\n",
    "population_dimension.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# State Code data\n",
    "logging.info('Getting state codes data.') \n",
    "state_codes_staging = spark.read.text(\"s3a://de-capstone-anil/state_codes.txt\")\n",
    "@udf\n",
    "def split_state_code(row):\n",
    "    return row.split(' ')[-1]\n",
    "@udf\n",
    "def split_state_name(row):\n",
    "    return \" \".join(row.split(' ')[0:-1])\n",
    "state_codes_dimension = state_codes_staging.withColumn('state_code', split_state_code(state_codes_staging['value'])).withColumn('state_name', split_state_name(state_codes_staging['value'])).drop('value')\n",
    "state_codes_dimension.createOrReplaceTempView('state_codes_dimension')\n",
    "logging.info('State Codes fetched.')\n",
    "\n",
    "state_codes_dimension.write.parquet(\"State_Codes\", \"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(state_code='AL', state_name='Alabama')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This cell be could be utilised for ensuring proper data load\n",
    "state_codes_dimension.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Accidents data\n",
    "logging.info('Starting Accidents data read.') \n",
    "get_time()\n",
    "accidents_staging = spark.read.csv(\"s3a://de-capstone-anil/US_Accidents_May19.csv\", header=True)\n",
    "accidents_staging.createOrReplaceTempView('Accidents_staging_view')\n",
    "logging.info('Accidents data staged. Time taken: {} Row count: {}'.format(get_time(), accidents_staging.count()))\n",
    "\n",
    "logging.info('Cleaning Data and selecting necessary columns.')\n",
    "countries = accidents_staging.groupby('country').count().collect()\n",
    "logging.info(\"Selecting only those rows with country set to US.\")\n",
    "for country in countries:\n",
    "    logging.info('Country: {} Count: {}'.format(country['country'], country('name')))\n",
    "accidents_temp = accidents_staging.filter(accidents_staging['country'] == 'US')\n",
    "logging.info('After filtering, Row count: {}'.format(accidents_temp.count()))\n",
    "\n",
    "accidents_temp = accidents_temp.filter(accidents_staging['Start_Time'].startswith('2016') | accidents_staging['Start_Time'].startswith('2017') | accidents_staging['Start_Time'].startswith('2018') | accidents_staging['Start_Time'].startswith('2019'))\n",
    "logging.info('Filtered data based on year.')\n",
    "logging.info('After filtering, Row count: {}'.format(accidents_temp.count()))\n",
    "\n",
    "accidents_dimension = accidents_temp.withColumn('year', substring(accidents_temp['start_time'], 0, 4).cast(IntegerType())).select('id','severity','start_time','end_time','description','side','state','weather_condition', 'year').dropDuplicates()\n",
    "accidents_dimension.createOrReplaceTempView('accidents_dimension')\n",
    "logging.info('Accidents dimension created. ')\n",
    "\n",
    "accidents_dimension.write.parquet(\"Accidents_Dimension\", \"overwrite\", \"state\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "accidents_staging.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Vehicles data\n",
    "logging.info('Starting Vehicles data read.') \n",
    "get_time()\n",
    "vehicle_staging = spark.read.text(\"s3a://de-capstone-anil/vehicles_per_1000.txt\")\n",
    "logging.info('Vehicles data staged. Time taken: {} Row count: {}'.format(get_time(), vehicle_staging.count()))\n",
    "logging.info('Applying data transformation.')\n",
    "@udf\n",
    "def get_state_vehicle_data(row):\n",
    "    split = row.split(' ')\n",
    "    state_name = \" \".join(split[1:-1])\n",
    "    count = split[-1]\n",
    "    return state_name\n",
    "\n",
    "@udf\n",
    "def get_count_vehicle_data(row):\n",
    "    split = row.split(' ')\n",
    "    count = split[-1]\n",
    "    return count\n",
    "\n",
    "vehicle_dimension = vehicle_staging.withColumn('state', get_state_vehicle_data(vehicle_staging['value'])).withColumn('2017', get_count_vehicle_data(vehicle_staging['value']).cast(IntegerType())).drop('value')\n",
    "\n",
    "logging.info('Extrapolating the data based on statistics.')\n",
    "increase_2018_17 = 272.1/268.3\n",
    "increase_2019_17 = 276/268.3\n",
    "logging.info('Assuming the count in 2016 decreases by the same fraction as it increased in 2017.')\n",
    "decrease_2016_17 = 1/increase_2018_17\n",
    "\n",
    "vehicle_dimension = vehicle_dimension.withColumn('2016', vehicle_dimension['2017']*decrease_2016_17).withColumn('2018', vehicle_dimension['2017']*increase_2018_17).withColumn('2019', vehicle_dimension['2017']*increase_2019_17)\n",
    "\n",
    "vehicle_dimension.write.parquet(\"Vehicles_Dimension\", \"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "vehicle_staging.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "logging.info('Creating Location Dimension Table.')\n",
    "location_dimension = spark.sql('SELECT street, city, county, state FROM accidents_staging_view GROUP BY street, city, county, state')\n",
    "logging.info('Number of records: {}'.format(location_dimension.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Joining state data with Accidents dimension\n",
    "accidents_states = accidents_dimension.join(state_codes_dimension, on=[accidents_temp['state'] == state_codes_dimension['state_code']], how='full').drop('value', 'state_code')\n",
    "logging.info('Added state names.')\n",
    "accidents_temp.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Creating a Vehicle_Population dimension\n",
    "Vehicles_Population = vehicle_dimension.join(population_dimension, on=[vehicle_dimension['state'] == population_dimension['name']], how='full').withColumn('vehicles_2019', ceil(vehicle_dimension['2019']*population_dimension['population_2019']/1000)).withColumn('vehicles_2018', ceil(vehicle_dimension['2018']*population_dimension['population_2018']/1000)).withColumn('vehicles_2017', ceil(vehicle_dimension['2017']*population_dimension['population_2017']/1000)).withColumn('vehicles_2016', ceil(vehicle_dimension['2016']*population_dimension['population_2016']/1000)).drop('2016').drop('2017').drop('2018').drop('2019').drop(vehicle_dimension['state'])\n",
    "logging.info('Joined Vehicle dimension with Population dimension. Row count: {}'.format(Vehicles_Population.count()))\n",
    "\n",
    "Vehicles_Population.write.parquet(\"Vehicles_Population\", \"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "Vehicles_Population.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Creating the fact table\n",
    "accidents_fact = accidents_states.groupby('state_name').agg({'id':'count'}).join(Vehicles_Population, on=[Vehicles_Population['name'] == accidents_states['state_name']], how='inner').drop(Vehicles_Population['name'])\n",
    "logging.info('Created fact table dataframe.')\n",
    "logging.info('Sample row: {}'.format(accidents_fact.head()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
